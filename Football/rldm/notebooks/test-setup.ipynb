{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some commands to help you troubleshoot your setup\n",
    "# There is no action items that you need to bring up to the teaching staff\n",
    "# Remember, the job is for you to complete, we only provide you a potential\n",
    "# path, but you don't have to take it. We're also not going to help troubleshoot\n",
    "# installation issues. For that, feel free to use online resources depening\n",
    "# on the type of issue (or the application) that you are seeing.\n",
    "\n",
    "# Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import argparse\n",
    "import gfootball.env as football_env\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tempfile\n",
    "import gym\n",
    "from gfootball import env as fe\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from IPython.display import HTML\n",
    "\n",
    "from rldm.utils import gif_tools as gt\n",
    "from rldm.utils import football_tools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import pybullet_envs\n",
    "import argparse\n",
    "import gfootball.env as football_env\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray import tune\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tempfile\n",
    "import gym\n",
    "from gfootball import env as fe\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from gym import wrappers\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from IPython.display import HTML\n",
    "\n",
    "from rldm.utils import gif_tools as gt\n",
    "from rldm.utils import football_tools as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We expect the following packages:\n",
    "\n",
    "# gfootball                     2.10.1\n",
    "# ray                           1.6.0\n",
    "# tensorflow                    2.6.0\n",
    "# tensorflow-estimator          2.6.0\n",
    "# tensorboard                   2.6.0\n",
    "# tensorboard-data-server       0.6.1\n",
    "# tensorboard-plugin-wit        1.8.0\n",
    "# tensorboardX                  2.4\n",
    "# torch                         1.9.0+cu111\n",
    "# torchaudio                    0.9.0\n",
    "# torchvision                   0.10.0+cu111\n",
    "# rldm                          1.0          /mnt\n",
    "\n",
    "# If you don't have the same packages, then maybe you don't have GPUs? Or you didn't use the Docker image provided?\n",
    "\n",
    "# No need to reach out if you don't have the same packages. This is only for your information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gfootball                     2.10.2\n",
      "ray                           1.6.0\n",
      "tensorflow                    2.8.0\n",
      "tensorflow-io-gcs-filesystem  0.24.0\n",
      "tensorboard                   2.8.0\n",
      "tensorboard-data-server       0.6.1\n",
      "tensorboard-plugin-wit        1.8.1\n",
      "tensorboardX                  2.5\n",
      "torch                         1.9.0+cu111\n",
      "torchaudio                    0.9.0\n",
      "torchvision                   0.10.0+cu111\n",
      "rldm                          1.0                 /mnt\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep -i gfootball\n",
    "!pip list | grep -i ray\n",
    "!pip list | grep -i tensorflow\n",
    "!pip list | grep -i tensorboard\n",
    "!pip list | grep -i torch\n",
    "!pip list | grep -i rldm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command below will fail if you don't have a GPU.\n",
    "# There is no action item for you if you don't have a GPU and the command above fails.\n",
    "\n",
    "# If you do have a GPU and it is not showing on the output of the command above,\n",
    "# then something is wrong. Did you follow all the steps carefully? Are you passing the right\n",
    "# GPU command to docker when starting? These are some initial ideas for you to troubleshoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 10 13:32:38 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 531.14       CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce GTX 1070         On | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   54C    P8               17W / 151W|    556MiB /  8192MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A        31      G   /Xwayland                                 N/A      |\r\n",
      "|    0   N/A  N/A        32      G   /Xwayland                                 N/A      |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of vCPUs on the system! Having lots of vCPUs can be\n",
    "# very helpful specially for this project. If you have multiple systems,\n",
    "# you may want to consider the system with the highest number of vCPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\r\n"
     ]
    }
   ],
   "source": [
    "!grep -c 'processor' /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For you to become aware of the memory available on your system.\n",
    "# If you are considering using complex models, such as models with\n",
    "# Recurrent cells, such as LSTMs, GRUs, or AttentionNets, then make\n",
    "# sure you have sufficient memory for your model. \"Sufficient\" here\n",
    "# depends on the actual model you end up using (number of params, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:           15Gi       2.0Gi       180Mi       3.0Mi        13Gi        13Gi\r\n",
      "Swap:         4.0Gi       2.0Mi       4.0Gi\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command below helps you become aware of the available\n",
    "# free space on your system. This can become important if you\n",
    "# are saving lots of checkpoints as you train. So make sure you\n",
    "# have enough for whatever you're planning to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "overlay        1007G   58G  899G   7% /\r\n",
      "tmpfs            64M     0   64M   0% /dev\r\n",
      "shm              64M     0   64M   0% /dev/shm\r\n",
      "drvfs           466G  465G  237M 100% /mnt\r\n",
      "/dev/sdd       1007G   58G  899G   7% /etc/hosts\r\n",
      "none            466G  465G  237M 100% /usr/bin/nvidia-smi\r\n",
      "none            7.8G     0  7.8G   0% /usr/lib/x86_64-linux-gnu/libcuda.so.1\r\n",
      "none            7.8G     0  7.8G   0% /usr/lib/x86_64-linux-gnu/libdxcore.so\r\n",
      "none            7.8G     0  7.8G   0% /dev/dxg\r\n",
      "tmpfs           7.8G     0  7.8G   0% /proc/acpi\r\n",
      "tmpfs           7.8G     0  7.8G   0% /sys/firmware\r\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following command helps you verify\n",
    "# whether PyTorch is seeing your GPU or not.\n",
    "# It should say True if you do have a GPU and you\n",
    "# are using the provided Docker images, and if\n",
    "# you followed the instructions carefully\n",
    "\n",
    "# This is info for you only, to help you on your path\n",
    "# Feel free to try something else on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
